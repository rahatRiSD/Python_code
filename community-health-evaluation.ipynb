{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8cf6bd-2d17-44d5-8b1d-3261ec6a6e85",
   "metadata": {},
   "source": [
    "# Define Data Loading and Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8d739e-0614-47a3-8cfa-3914d30236b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from lightgbm) (2.0.2)\n",
      "Requirement already satisfied: scipy in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from lightgbm) (1.14.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ea366d-4160-4ea0-a504-c8e5f5a52aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp312-cp312-macosx_11_0_universal2.whl.metadata (1.4 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from catboost) (3.9.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from catboost) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from catboost) (1.14.1)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from matplotlib->catboost) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from matplotlib->catboost) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from matplotlib->catboost) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from matplotlib->catboost) (3.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from plotly->catboost) (1.34.1)\n",
      "Downloading catboost-1.2.8-cp312-cp312-macosx_11_0_universal2.whl (27.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: plotly, graphviz, catboost\n",
      "Successfully installed catboost-1.2.8 graphviz-0.20.3 plotly-6.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d219ecb3-0fd0-40d8-bfca-ea6ddb27d03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from imbalanced-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from imbalanced-learn) (1.5.2)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1648bec-e3c1-4987-a860-0a339c7d5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, auc, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03820d93-834e-4529-a241-f94f1a8c43c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'prognosis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prognosis'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/rahatrihan/Desktop/AIUB/data sci/community_health_evaluation_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 🔽 Call the function and retrieve the data\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m X, y, df \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal missing values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_values\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Display class imbalance\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m class_distribution \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprognosis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mClass distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_distribution)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prognosis'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the disease dataset.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"Total missing values: {missing_values.sum()}\")\n",
    "    \n",
    "    # Display class imbalance\n",
    "    class_distribution = df['prognosis'].value_counts()\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(class_distribution)\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    ax = sns.countplot(x='prognosis', data=df)\n",
    "    plt.title('Disease Class Distribution', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('prognosis', axis=1)\n",
    "    y = df['prognosis']\n",
    "    \n",
    "    # Print feature value distribution (top 5 features)\n",
    "    print(\"\\nFeature value distribution (top 5 features):\")\n",
    "    for col in X.columns[:5]:\n",
    "        print(f\"{col}: {X[col].value_counts().to_dict()}\")\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# 🔽 Provide the full path to your dataset here\n",
    "file_path = '/Users/rahatrihan/Desktop/AIUB/data sci/community_health_evaluation_dataset.csv'\n",
    "\n",
    "# 🔽 Call the function and retrieve the data\n",
    "X, y, df = load_and_preprocess_data(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32253ca0-a165-4129-8a78-d5c68981962f",
   "metadata": {},
   "source": [
    "# Set Up Plotting Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d375083-3fbf-4df7-9e9a-bba66c631f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#ff9896', '#98df8a']\n",
    "sns.set_palette(sns.color_palette(colors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823a2bd-2e1a-4395-9146-5c562c230f63",
   "metadata": {},
   "source": [
    "# Define Data Loading and Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28113258-3fe6-422b-a15c-3df08ff27470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the disease dataset\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)  # Load the actual dataset\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"Missing values: {missing_values.sum()}\")\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    class_distribution = df['prognosis'].value_counts()\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(class_distribution)\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    ax = sns.countplot(x='prognosis', data=df)\n",
    "    plt.title('Disease Class Distribution', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('prognosis', axis=1)\n",
    "    y = df['prognosis']\n",
    "    \n",
    "    # Print feature characteristics for understanding binary nature\n",
    "    print(\"\\nFeature value distribution (top 5 features):\")\n",
    "    for col in X.columns[:5]:\n",
    "        print(f\"{col}: {X[col].value_counts().to_dict()}\")\n",
    "    \n",
    "    return X, y, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985613c-6b65-45eb-a656-eeb8587288bf",
   "metadata": {},
   "source": [
    "# Define Feature Selection with Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f574e0c-6822-4116-b6a2-3854ff996fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_mutual_info(X, y, top_n=30):\n",
    "    \"\"\"\n",
    "    Select top N features using Mutual Information\n",
    "    \"\"\"\n",
    "    mutual_info = mutual_info_classif(X, y)\n",
    "    feature_scores = pd.Series(mutual_info, index=X.columns)\n",
    "    \n",
    "    # Select the top N features based on the mutual information score\n",
    "    top_features = feature_scores.nlargest(top_n).index.tolist()\n",
    "    print(f\"Top {top_n} selected features based on Mutual Information:\")\n",
    "    for i, feature in enumerate(top_features, 1):\n",
    "        print(f\"{i}. {feature} (Score: {feature_scores[feature]:.4f})\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    feature_scores.nlargest(top_n).sort_values().plot(kind='barh')\n",
    "    plt.title(f'Top {top_n} Features by Mutual Information', fontsize=16)\n",
    "    plt.xlabel('Mutual Information Score', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return X[top_features], top_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305277d5-dbbb-42b5-ad98-fcd0d022d673",
   "metadata": {},
   "source": [
    "# Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b510fd-e6e2-4828-ba9a-e39fc59eddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_class_imbalance(X, y, method='combined'):\n",
    "    \"\"\"\n",
    "    Handle class imbalance using various techniques\n",
    "    \"\"\"\n",
    "    if method == 'none':\n",
    "        return X, y\n",
    "    \n",
    "    if method == 'smote':\n",
    "        print(\"Applying SMOTE to handle class imbalance...\")\n",
    "        sampler = SMOTE(random_state=42, k_neighbors=5)\n",
    "    elif method == 'adasyn':\n",
    "        print(\"Applying ADASYN to handle class imbalance...\")\n",
    "        sampler = ADASYN(random_state=42, n_neighbors=5)\n",
    "    elif method == 'undersample':\n",
    "        print(\"Applying Random Undersampling to handle class imbalance...\")\n",
    "        sampler = RandomUnderSampler(random_state=42)\n",
    "    elif method == 'combined':\n",
    "        print(\"Applying combined sampling approach...\")\n",
    "        \n",
    "        # First apply undersampling to extreme majority classes\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        max_count = class_counts.max()\n",
    "        min_count = class_counts.min()\n",
    "        \n",
    "        if max_count / min_count > 10:\n",
    "            print(\"Detected extreme imbalance. First applying undersampling...\")\n",
    "            under_sampler = RandomUnderSampler(\n",
    "                sampling_strategy={cls: min(count, max_count // 5) for cls, count in class_counts.items() if count > max_count // 2},\n",
    "                random_state=42\n",
    "            )\n",
    "            X, y = under_sampler.fit_resample(X, y)\n",
    "        \n",
    "        print(\"Applying SMOTE for final balancing...\")\n",
    "        sampler = SMOTE(random_state=42, k_neighbors=min(5, min_count-1))\n",
    "    \n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    print(f\"Shape before resampling: {X.shape}\")\n",
    "    print(f\"Shape after resampling: {X_resampled.shape}\")\n",
    "    \n",
    "    # Plot the resampled class distribution\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.countplot(x=y_resampled)\n",
    "    plt.title('Resampled Class Distribution', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('resampled_class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437730b-6c3e-4812-a558-f5e2fd126de1",
   "metadata": {},
   "source": [
    "#  Create List of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36a7124-9a24-447c-9272-44b04e892047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_list():\n",
    "    \"\"\"\n",
    "    Create a list of models suitable for binary features with categorical targets\n",
    "    \"\"\"\n",
    "    model_list = [\n",
    "        ('Logistic Regression', LogisticRegression(C=1.0, solver='saga', penalty='l2', max_iter=2000, class_weight='balanced', random_state=42)),\n",
    "        ('Random Forest', RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', bootstrap=True, class_weight='balanced', oob_score=True, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(learning_rate=0.05, n_estimators=200, max_depth=6, objective='multi:softprob', subsample=0.8, colsample_bytree=0.8, gamma=0.1, min_child_weight=1, use_label_encoder=False, eval_metric='mlogloss', random_state=42)),\n",
    "        ('Decision Tree', DecisionTreeClassifier(max_depth=10, min_samples_split=5, min_samples_leaf=2, criterion='entropy', class_weight='balanced', random_state=42)),\n",
    "        ('SVM', SVC(C=10.0, kernel='rbf', gamma='scale', decision_function_shape='ovr', probability=True, class_weight='balanced', random_state=42)),\n",
    "        ('Gradient Boosting', GradientBoostingClassifier(learning_rate=0.05, n_estimators=200, max_depth=5, min_samples_split=10, subsample=0.8, random_state=42)),\n",
    "        # New models good for binary features\n",
    "        ('Bernoulli NB', BernoulliNB(alpha=1.0)), # Specifically designed for binary features\n",
    "        ('KNN', KNeighborsClassifier(n_neighbors=5, weights='distance', metric='hamming')), # Hamming distance for binary features\n",
    "        ('AdaBoost', AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "        ('Extra Trees', ExtraTreesClassifier(n_estimators=200, max_depth=15, min_samples_split=5, class_weight='balanced', random_state=42)),\n",
    "        ('Ridge Classifier', RidgeClassifier(alpha=1.0, class_weight='balanced', random_state=42)),\n",
    "        ('MLP Neural Network', MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', alpha=0.0001, max_iter=500, random_state=42)),\n",
    "        ('LightGBM', LGBMClassifier(n_estimators=200, learning_rate=0.05, num_leaves=31, max_depth=-1, random_state=42, verbose=-1)),\n",
    "        ('CatBoost', CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, loss_function='MultiClass', random_state=42, verbose=0)),\n",
    "        ('Voting Classifier', VotingClassifier(estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)), \n",
    "            ('xgb', XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=42)), \n",
    "            ('lgbm', LGBMClassifier(n_estimators=100, random_state=42, verbose=-1))\n",
    "        ], voting='soft', n_jobs=-1))\n",
    "    ]\n",
    "    \n",
    "    return model_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90592b3c-d54f-4520-8ed4-c8eb69449e8e",
   "metadata": {},
   "source": [
    "# Create Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "988cb66d-d534-4b2a-8106-cfc7dd5888da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacking_model():\n",
    "    \"\"\"\n",
    "    Create an optimized stacking ensemble model specifically for binary features\n",
    "    \"\"\"\n",
    "    base_models = [\n",
    "        ('lr', LogisticRegression(C=1.0, solver='saga', penalty='l2', max_iter=1000, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', class_weight='balanced', random_state=42)),\n",
    "        ('xgb', XGBClassifier(learning_rate=0.05, n_estimators=200, max_depth=6, subsample=0.8, colsample_bytree=0.8, gamma=0.1, min_child_weight=1, use_label_encoder=False, eval_metric='mlogloss', random_state=42)),\n",
    "        ('bnb', BernoulliNB(alpha=1.0)),  # Adding Bernoulli NB as it's good for binary features\n",
    "        ('et', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    meta_model = LogisticRegression(C=1.0, solver='saga', max_iter=1000, random_state=42)\n",
    "    \n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        cv=5,\n",
    "        stack_method='predict_proba',\n",
    "        n_jobs=-1,\n",
    "        passthrough=True\n",
    "    )\n",
    "    \n",
    "    return stacking_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07174f8-fd46-453a-8470-3079b89e119f",
   "metadata": {},
   "source": [
    "# Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad149426-058b-4c8b-8c9d-5ff322bcbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_val, y_train, y_val, top_features):\n",
    "    \"\"\"\n",
    "    Train multiple models with optimized parameters and evaluate their performance\n",
    "    Record training and inference times\n",
    "    \"\"\"\n",
    "    model_list = create_model_list()\n",
    "    model_list.append(('Stacking Ensemble', create_stacking_model()))\n",
    "    \n",
    "    results = {}\n",
    "    all_predictions = {}\n",
    "    \n",
    "    print(\"\\n{:<20} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12}\".format(\n",
    "        \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC\", \"Train Time\", \"Infer Time\"))\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for name, model in model_list:\n",
    "        # Training time\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Inference time\n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_val)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_prob = model.predict_proba(X_val)\n",
    "        else:\n",
    "            # For models that don't have predict_proba, use decision_function if available\n",
    "            if hasattr(model, 'decision_function'):\n",
    "                decisions = model.decision_function(X_val)\n",
    "                # Convert decision function to probabilities\n",
    "                if decisions.ndim == 1:\n",
    "                    y_prob = np.vstack([(1 - decisions), decisions]).T\n",
    "                else:\n",
    "                    y_prob = np.exp(decisions) / np.sum(np.exp(decisions), axis=1, keepdims=True)\n",
    "            else:\n",
    "                # For models without probability output\n",
    "                y_prob = np.zeros((len(y_val), len(np.unique(y_val))))\n",
    "                for i, pred in enumerate(y_pred):\n",
    "                    y_prob[i, pred] = 1\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        \n",
    "        # Calculate AUC with proper handling\n",
    "        try:\n",
    "            auc_score = roc_auc_score(y_val, y_prob, multi_class='ovr', average='macro')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate AUC for {name}: {e}\")\n",
    "            auc_score = 0.0\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc_score,\n",
    "            'train_time': train_time,\n",
    "            'inference_time': inference_time,\n",
    "            'y_pred': y_pred,\n",
    "            'y_prob': y_prob\n",
    "        }\n",
    "        \n",
    "        print(\"{:<20} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f}\".format(\n",
    "            name, accuracy, precision, recall, f1, auc_score, train_time, inference_time))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c6b3d-db9e-452c-a02c-a1e1185dad03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory /kaggle/input/facedataset/Face Dataset/DataSet. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m DATASET_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/facedataset/Face Dataset/DataSet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_data \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(\n\u001b[1;32m     20\u001b[0m     DATASET_DIR,\n\u001b[1;32m     21\u001b[0m     label_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     23\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mfile_paths\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Define label mappings\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/image_dataset_utils.py:329\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    325\u001b[0m image_paths, labels \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mget_training_or_validation_split(\n\u001b[1;32m    326\u001b[0m     image_paths, labels, validation_split, subset\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_paths:\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed formats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWLIST_FORMATS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    334\u001b[0m dataset \u001b[38;5;241m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[1;32m    335\u001b[0m     image_paths\u001b[38;5;241m=\u001b[39mimage_paths,\n\u001b[1;32m    336\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: No images found in directory /kaggle/input/facedataset/Face Dataset/DataSet. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# =============================\n",
    "# Configuration\n",
    "# =============================\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "DATASET_DIR = \"/kaggle/input/facedataset/Face Dataset/DataSet\"\n",
    "\n",
    "# =============================\n",
    "# Load Data\n",
    "# =============================\n",
    "train_data = image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    label_mode=None,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "file_paths = train_data.file_paths\n",
    "\n",
    "# Define label mappings\n",
    "person_to_label = {'Muktadir': 0, 'Rahat': 1}\n",
    "emotion_to_label = {'Angry': 0, 'Natural': 1, 'Smile': 2}\n",
    "\n",
    "# =============================\n",
    "# Label Extraction Functions\n",
    "# =============================\n",
    "def extract_labels(file_path):\n",
    "    file_path_str = file_path.numpy().decode(\"utf-8\")\n",
    "    parts = file_path_str.split(os.path.sep)\n",
    "    person_name = parts[-3]\n",
    "    emotion_name = parts[-2]\n",
    "    person_label = person_to_label[person_name]\n",
    "    emotion_label = emotion_to_label[emotion_name]\n",
    "    return np.int32(person_label), np.int32(emotion_label)\n",
    "\n",
    "def get_labels(file_path):\n",
    "    person, emotion = tf.py_function(func=extract_labels,\n",
    "                                     inp=[file_path],\n",
    "                                     Tout=[tf.int32, tf.int32])\n",
    "    person.set_shape(())\n",
    "    emotion.set_shape(())\n",
    "    return person, emotion\n",
    "\n",
    "file_path_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "label_ds = file_path_ds.map(get_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "label_ds = label_ds.batch(BATCH_SIZE)\n",
    "train_ds = tf.data.Dataset.zip((train_data, label_ds)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# =============================\n",
    "# Load MobileNetV2 Model\n",
    "# =============================\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze base model\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Two outputs\n",
    "person_output = Dense(2, activation='softmax', name='person_output')(x)\n",
    "emotion_output = Dense(3, activation='softmax', name='emotion_output')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=[person_output, emotion_output])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'person_output': 'sparse_categorical_crossentropy',\n",
    "        'emotion_output': 'sparse_categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'person_output': 'accuracy',\n",
    "        'emotion_output': 'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# =============================\n",
    "# Train Model\n",
    "# =============================\n",
    "history = model.fit(train_ds, epochs=EPOCHS)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"face_emotion_mobilenet2.h5\")\n",
    "print(\"MobileNetV2-based model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6583395,
     "sourceId": 10633311,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

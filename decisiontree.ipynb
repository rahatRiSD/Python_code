{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de818386-0f5b-4a00-9375-bc7998bfdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame from the dataset\n",
    "data = {\n",
    "    \"Buying\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "               \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "               \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Price\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "              \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "              \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Maintenance\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "                    \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "                    \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Doors\": [2, 2, 3, 3, 3, \"5more\", \"5more\", \"5more\", \"5more\", \"5more\", \"5more\", \n",
    "              \"5more\", \"5more\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"5more\", \"5more\", \n",
    "              \"5more\", \"5more\", 2, 2, 3, 3, 3, 4],\n",
    "    \"Persons\": [4, \"more\", 4, \"more\", \"more\", 2, 2, 4, 4, 4, 4, \"more\", \"more\", 4, 4, \n",
    "                \"more\", \"more\", \"4\", 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, \"more\"],\n",
    "    \"Lug_Boot\": [\"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"small\", \"med\", \"big\", \n",
    "                 \"small\", \"med\", \"small\", \"med\", \"med\", \"small\", \"med\", \"big\", \"small\", \n",
    "                 \"big\", \"small\", \"small\", \"small\", \"small\", \"med\", \"small\", \"med\", \"small\", \n",
    "                 \"big\", \"small\"],\n",
    "    \"Safety\": [\"vgood\", \"vgood\", \"vgood\", \"vgood\", \"vgood\", \"unacc\", \"unacc\", \"unacc\", \n",
    "               \"unacc\", \"unacc\", \"unacc\", \"unacc\", \"unacc\", \"good\", \"good\", \"good\", \"good\", \n",
    "               \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"acc\", \"acc\", \n",
    "               \"acc\", \"acc\", \"acc\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Target class\n",
    "target = \"Safety\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fade2624-436e-4a4c-bafd-7ecff3fbd0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rahatrihan/miniforge3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a78d2e-4215-4e64-bc9b-e4b563c89bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_entropy(df, target):\n",
    "    \"\"\"Calculate entropy of the target class.\"\"\"\n",
    "    class_counts = df[target].value_counts()\n",
    "    probabilities = class_counts / len(df)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def split_data(df, attribute):\n",
    "    \"\"\"Split dataset by the given attribute.\"\"\"\n",
    "    return df.groupby(attribute)\n",
    "\n",
    "def best_split(df, attributes, target):\n",
    "    \"\"\"Choose the best attribute to split on.\"\"\"\n",
    "    base_entropy = calculate_entropy(df, target)\n",
    "    best_info_gain = -1\n",
    "    best_attribute = None\n",
    "    \n",
    "    for attribute in attributes:\n",
    "        subsets = split_data(df, attribute)\n",
    "        weighted_entropy = 0\n",
    "        \n",
    "        for _, subset in subsets:\n",
    "            weighted_entropy += (len(subset) / len(df)) * calculate_entropy(subset, target)\n",
    "        \n",
    "        info_gain = base_entropy - weighted_entropy\n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain = info_gain\n",
    "            best_attribute = attribute\n",
    "    \n",
    "    return best_attribute\n",
    "\n",
    "def tdidt(df, attributes, target, max_depth=5, depth=0):\n",
    "    \"\"\"Build a decision tree using TDIDT.\"\"\"\n",
    "    # If all instances are of the same class, return that class\n",
    "    if df[target].nunique() == 1:\n",
    "        return df[target].iloc[0]\n",
    "    \n",
    "    # If max depth is reached, return the most frequent class\n",
    "    if depth == max_depth:\n",
    "        return df[target].mode()[0]\n",
    "    \n",
    "    # Choose the best attribute to split on\n",
    "    best_attribute = best_split(df, attributes, target)\n",
    "    \n",
    "    # Split the dataset based on the best attribute\n",
    "    tree = {best_attribute: {}}\n",
    "    subsets = split_data(df, best_attribute)\n",
    "    \n",
    "    for value, subset in subsets:\n",
    "        tree[best_attribute][value] = tdidt(subset, [attr for attr in attributes if attr != best_attribute], target, max_depth, depth + 1)\n",
    "    \n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1753801-da8c-42cb-9a9d-27b299c04458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Take First):\n",
      "{'Doors': {2: {'Lug_Boot': {'big': 'vgood', 'med': 'good', 'small': 'acc'}}, 3: {'Buying': {'low': 'acc', 'med': 'vgood'}}, 4: 'acc', '4': 'good', '5more': {'Lug_Boot': {'big': 'unacc', 'med': 'unacc', 'small': {'Persons': {4: {'Buying': {'low': {'Price': {'low': 'good'}}}}, 'more': 'unacc'}}}}}}\n"
     ]
    }
   ],
   "source": [
    "# Define the list of attributes and build the decision tree using TDIDT\n",
    "attributes = [\"Buying\", \"Price\", \"Maintenance\", \"Doors\", \"Persons\", \"Lug_Boot\"]\n",
    "tree_take_first = tdidt(df, attributes, target)\n",
    "\n",
    "print(\"Decision Tree (Take First):\")\n",
    "print(tree_take_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb8b57c-8ad9-4405-a674-18d34e12ca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree (Take Last):\n",
      "{'Doors': {2: {'Lug_Boot': {'big': 'vgood', 'med': 'good', 'small': 'acc'}}, 3: {'Maintenance': {'low': 'acc', 'med': 'vgood'}}, 4: 'acc', '4': 'good', '5more': {'Lug_Boot': {'big': 'unacc', 'med': 'unacc', 'small': {'Persons': {4: {'Maintenance': {'low': {'Price': {'low': 'good'}}}}, 'more': 'unacc'}}}}}}\n"
     ]
    }
   ],
   "source": [
    "# Reversing the attributes to simulate \"Take Last\" approach\n",
    "attributes_last = list(reversed(attributes))\n",
    "tree_take_last = tdidt(df, attributes_last, target)\n",
    "\n",
    "print(\"\\nDecision Tree (Take Last):\")\n",
    "print(tree_take_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55cf6ece-f678-419f-875b-f8e3fca589d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rules(tree, parent_rule=None):\n",
    "    if parent_rule is None:\n",
    "        parent_rule = []  # Initialize as an empty list when it's the first call\n",
    "    \n",
    "    rules = []\n",
    "    \n",
    "    # Check if the current node is a leaf node (dictionary with only one key, which is the class label)\n",
    "    if isinstance(tree, str):  # Leaf node, return class label\n",
    "        rules.append(\" AND \".join(parent_rule) + f\" => {tree}\")\n",
    "    else:\n",
    "        # If it's not a leaf, it’s a decision node (has attributes to split on)\n",
    "        attribute = list(tree.keys())[0]\n",
    "        for value, subtree in tree[attribute].items():\n",
    "            rule = parent_rule + [f\"{attribute} = {value}\"]\n",
    "            rules += extract_rules(subtree, rule)  # Recurse for the subtree\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54edf7c5-25e2-4809-8307-a491fdc392ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2779101741.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\",\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset\n",
    "data = {\n",
    "    \"Buying\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "               \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26bdc9d8-e56f-40c6-a4db-5fab8c1212d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Index is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Apply TDIDT on the dataset\u001b[39;00m\n\u001b[1;32m     98\u001b[0m attributes \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Exclude the target column\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m tree_take_first \u001b[38;5;241m=\u001b[39m \u001b[43mtdidt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m tree_take_last \u001b[38;5;241m=\u001b[39m tdidt(df, attributes[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], target)  \u001b[38;5;66;03m# Reverse the order of attributes for \"Take Last\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Extract rules for \"Take First\" and \"Take Last\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 63\u001b[0m, in \u001b[0;36mtdidt\u001b[0;34m(data, attributes, target)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[target]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# If no attributes are left to split on, return the most common class\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attributes:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[target]\u001b[38;5;241m.\u001b[39mmode()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Choose the best attribute based on information gain\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3190\u001b[0m, in \u001b[0;36mIndex.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3188\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   3189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m-> 3190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3193\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Index is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Dataset\n",
    "data = {\n",
    "    \"Buying\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "               \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "               \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Price\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "              \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "              \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Maintenance\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "                    \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "                    \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Doors\": [2, 2, 3, 3, 3, \"5more\", \"5more\", \"5more\", \"5more\", \"5more\", \"5more\", \n",
    "              \"5more\", \"5more\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"5more\", \"5more\", \n",
    "              \"5more\", \"5more\", 2, 2, 3, 3, 3, 4],\n",
    "    \"Persons\": [4, \"more\", 4, \"more\", \"more\", 2, 2, 4, 4, 4, 4, \"more\", \"more\", 4, 4, \n",
    "                \"more\", \"more\", \"4\", 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, \"more\"],\n",
    "    \"Lug_Boot\": [\"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"small\", \"med\", \"big\", \n",
    "                 \"small\", \"med\", \"small\", \"med\", \"med\", \"small\", \"med\", \"big\", \"small\", \n",
    "                 \"big\", \"small\", \"small\", \"small\", \"small\", \"med\", \"small\", \"med\", \"small\", \n",
    "                 \"big\", \"small\"],\n",
    "    \"Safety\": [\"vgood\", \"vgood\", \"vgood\", \"vgood\", \"vgood\", \"unacc\", \"unacc\", \"unacc\", \n",
    "               \"unacc\", \"unacc\", \"unacc\", \"unacc\", \"unacc\", \"good\", \"good\", \"good\", \"good\", \n",
    "               \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"acc\", \"acc\", \n",
    "               \"acc\", \"acc\", \"acc\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Target class\n",
    "target = \"Safety\"\n",
    "\n",
    "# TDIDT Algorithm\n",
    "\n",
    "def entropy(data):\n",
    "    \"\"\"Calculate the entropy of a dataset.\"\"\"\n",
    "    class_counts = Counter(data)\n",
    "    total = len(data)\n",
    "    return -sum((count/total) * np.log2(count/total) for count in class_counts.values())\n",
    "\n",
    "def information_gain(data, attribute, target):\n",
    "    \"\"\"Calculate the information gain of a dataset for a given attribute.\"\"\"\n",
    "    total_entropy = entropy(data[target])\n",
    "    values = data[attribute].unique()\n",
    "    \n",
    "    weighted_entropy = 0\n",
    "    for value in values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        weighted_entropy += (len(subset) / len(data)) * entropy(subset)\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "def tdidt(data, attributes, target):\n",
    "    \"\"\"Implement the TDIDT algorithm for decision tree learning.\"\"\"\n",
    "    # If all data belongs to one class, return that class\n",
    "    if len(set(data[target])) == 1:\n",
    "        return data[target].iloc[0]\n",
    "    \n",
    "    # If no attributes are left to split on, return the most common class\n",
    "    if not attributes:\n",
    "        return data[target].mode()[0]\n",
    "    \n",
    "    # Choose the best attribute based on information gain\n",
    "    gains = [(attribute, information_gain(data, attribute, target)) for attribute in attributes]\n",
    "    best_attribute, _ = max(gains, key=lambda x: x[1])\n",
    "    \n",
    "    tree = {best_attribute: {}}\n",
    "    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n",
    "    \n",
    "    for value in data[best_attribute].unique():\n",
    "        subset = data[data[best_attribute] == value]\n",
    "        tree[best_attribute][value] = tdidt(subset, remaining_attributes, target)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# Extracting rules from the decision tree\n",
    "def extract_rules(tree, parent_rule=None):\n",
    "    if parent_rule is None:\n",
    "        parent_rule = []  # Initialize as an empty list when it's the first call\n",
    "    \n",
    "    rules = []\n",
    "    \n",
    "    # Check if the current node is a leaf node (dictionary with only one key, which is the class label)\n",
    "    if isinstance(tree, str):  # Leaf node, return class label\n",
    "        rules.append(\" AND \".join(parent_rule) + f\" => {tree}\")\n",
    "    else:\n",
    "        # If it's not a leaf, it’s a decision node (has attributes to split on)\n",
    "        attribute = list(tree.keys())[0]\n",
    "        for value, subtree in tree[attribute].items():\n",
    "            rule = parent_rule + [f\"{attribute} = {value}\"]\n",
    "            rules += extract_rules(subtree, rule)  # Recurse for the subtree\n",
    "    return rules\n",
    "\n",
    "# Apply TDIDT on the dataset\n",
    "attributes = df.columns[:-1]  # Exclude the target column\n",
    "tree_take_first = tdidt(df, attributes, target)\n",
    "tree_take_last = tdidt(df, attributes[::-1], target)  # Reverse the order of attributes for \"Take Last\"\n",
    "\n",
    "# Extract rules for \"Take First\" and \"Take Last\"\n",
    "rules_take_first = extract_rules(tree_take_first)\n",
    "rules_take_last = extract_rules(tree_take_last)\n",
    "\n",
    "# Print the rules\n",
    "print(\"\\nRules (Take First):\")\n",
    "for rule in rules_take_first:\n",
    "    print(rule)\n",
    "\n",
    "print(\"\\nRules (Take Last):\")\n",
    "for rule in rules_take_last:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6972a2b8-e0d9-4df1-9f3b-fae1463b9031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the dataset\n",
    "data = {\n",
    "    \"Buying\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "               \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "               \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Price\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "              \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "              \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Maintenance\": [\"med\", \"med\", \"med\", \"med\", \"med\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "                    \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "                    \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"],\n",
    "    \"Doors\": [2, 2, 3, 3, 3, \"5more\", \"5more\", \"5more\", \"5more\", \"5more\", \"5more\", \n",
    "              \"5more\", \"5more\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"5more\", \"5more\", \n",
    "              \"5more\", \"5more\", 2, 2, 3, 3, 3, 4],\n",
    "    \"Persons\": [4, \"more\", 4, \"more\", \"more\", 2, 2, 4, 4, 4, 4, \"more\", \"more\", 4, 4, \n",
    "                \"more\", \"more\", \"4\", 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, \"more\"],\n",
    "    \"Lug_Boot\": [\"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"small\", \"small\", \"big\", \n",
    "                 \"small\", \"med\", \"small\", \"med\", \"med\", \"small\", \"med\", \"big\", \"small\", \n",
    "                 \"big\", \"small\", \"small\", \"small\", \"small\", \"med\", \"small\", \"med\", \"small\", \n",
    "                 \"big\", \"small\"],\n",
    "    \"Safety Class\": [\"vgood\", \"vgood\", \"vgood\", \"vgood\", \"vgood\", \"unacc\", \"unacc\", \"unacc\", \n",
    "                     \"unacc\", \"unacc\", \"unacc\", \"unacc\", \"unacc\", \"good\", \"good\", \"good\", \"good\", \n",
    "                     \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"acc\", \"acc\", \n",
    "                     \"acc\", \"acc\", \"acc\"]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "df.to_csv('car_evaluation.csv', index=False)\n",
    "\n",
    "print(\"CSV file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cd79602-1f0b-40ad-99b6-db6732ed47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from CSV file:\n",
      "   Buying Price Maintenance  Doors Persons Lug_Boot Safety Class\n",
      "0     med   med         med      2       4      big        vgood\n",
      "1     med   med         med      2    more      big        vgood\n",
      "2     med   med         med      3       4      big        vgood\n",
      "3     med   med         med      3    more      big        vgood\n",
      "4     med   med         med      3    more      big        vgood\n",
      "5     low   low         low  5more       2      big        unacc\n",
      "6     low   low         low  5more       2      big        unacc\n",
      "7     low   low         low  5more       4    small        unacc\n",
      "8     low   low         low  5more       4    small        unacc\n",
      "9     low   low         low  5more       4      big        unacc\n",
      "10    low   low         low  5more       4    small        unacc\n",
      "11    low   low         low  5more    more      med        unacc\n",
      "12    low   low         low  5more    more    small        unacc\n",
      "13    low   low         low      4       4      med         good\n",
      "14    low   low         low      4       4      med         good\n",
      "15    low   low         low      4    more    small         good\n",
      "16    low   low         low      4    more      med         good\n",
      "17    low   low         low      4       4      big         good\n",
      "18    low   low         low      4       4    small         good\n",
      "19    low   low         low      4       4      big         good\n",
      "20    low   low         low  5more       4    small         good\n",
      "21    low   low         low  5more       4    small         good\n",
      "22    low   low         low  5more       4    small         good\n",
      "23    low   low         low  5more       4    small         good\n",
      "24    low   low         low      2       4      med         good\n",
      "25    low   low         low      2       4    small          acc\n",
      "26    low   low         low      3       4      med          acc\n",
      "27    low   low         low      3       4    small          acc\n",
      "28    low   low         low      3       4      big          acc\n",
      "29    low   low         low      4    more    small          acc\n",
      "\n",
      "Updated Data has been written to /Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your existing CSV file\n",
    "file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "\n",
    "# Read the existing CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the DataFrame (optional, just to check the contents)\n",
    "print(\"Data from CSV file:\")\n",
    "print(df)\n",
    "\n",
    "# If you want to modify the DataFrame, here is an example:\n",
    "# Let's say we add a new row to the DataFrame\n",
    "new_data = {\n",
    "    \"Buying\": [\"low\"],\n",
    "    \"Price\": [\"med\"],\n",
    "    \"Maintenance\": [\"low\"],\n",
    "    \"Doors\": [4],\n",
    "    \"Persons\": [4],\n",
    "    \"Lug_Boot\": [\"big\"],\n",
    "    \"Safety Class\": [\"good\"]\n",
    "}\n",
    "new_row = pd.DataFrame(new_data)\n",
    "\n",
    "# Append the new row to the existing DataFrame\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# Write the updated DataFrame back to the same CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"\\nUpdated Data has been written to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c75aadd5-e3fd-4649-80d6-10acbfca11dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Buying Price Maintenance Doors Persons Lug_Boot Safety Class\n",
      "0    med   med         med     2       4      big        vgood\n",
      "1    med   med         med     2    more      big        vgood\n",
      "2    med   med         med     3       4      big        vgood\n",
      "3    med   med         med     3    more      big        vgood\n",
      "4    med   med         med     3    more      big        vgood\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Print the first few rows of the data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a03e92-79f4-4654-8e21-42081ff9aa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file at /Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_data.csv was not found. Please check the file path.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Corrected path to the CSV file\n",
    "file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_data.csv'\n",
    "\n",
    "try:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Print the first few rows of the data\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {file_path} was not found. Please check the file path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d5e6f76-87b9-4c87-a9ab-8ef789f2d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Buying Price Maintenance Doors Persons Lug_Boot Safety Class\n",
      "0    med   med         med     2       4      big        vgood\n",
      "1    med   med         med     2    more      big        vgood\n",
      "2    med   med         med     3       4      big        vgood\n",
      "3    med   med         med     3    more      big        vgood\n",
      "4    med   med         med     3    more      big        vgood\n"
     ]
    }
   ],
   "source": [
    "    print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1c055-31e9-4288-89e4-9a9ffe220f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7da4d23-3c7a-47e2-8e4a-d7da50603ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c600a2-3e90-4ca4-97cb-ab8aee69b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Buying Price Maintenance Doors Persons Lug_Boot Safety Class\n",
      "0    med   med         med     2       4      big        vgood\n",
      "1    med   med         med     2    more      big        vgood\n",
      "2    med   med         med     3       4      big        vgood\n",
      "3    med   med         med     3    more      big        vgood\n",
      "4    med   med         med     3    more      big        vgood\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Corrected path to the CSV file\n",
    "file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "\n",
    "try:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Print the first few rows of the data\n",
    "    print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {file_path} was not found. Please check the file path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d025dd03-8ae6-46e7-a3ae-5caf55c093da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the first row: vgood\n",
      "Predictions on the dataset: 0     vgood\n",
      "1     vgood\n",
      "2     vgood\n",
      "3     vgood\n",
      "4     vgood\n",
      "5      good\n",
      "6      good\n",
      "7      good\n",
      "8      good\n",
      "9      good\n",
      "10     good\n",
      "11     good\n",
      "12     good\n",
      "13     good\n",
      "14     good\n",
      "15     good\n",
      "16     good\n",
      "17     good\n",
      "18     good\n",
      "19     good\n",
      "20     good\n",
      "21     good\n",
      "22     good\n",
      "23     good\n",
      "24     good\n",
      "25     good\n",
      "26     good\n",
      "27     good\n",
      "28     good\n",
      "29     good\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Step 1: Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain\n",
    "def calculate_information_gain(data, feature):\n",
    "    # Calculate the entropy of the entire dataset\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    # Group by the feature and calculate weighted entropy of each group\n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    # Information Gain = Total Entropy - Weighted Entropy of Feature\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: Build the Decision Tree\n",
    "class TDIDT:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        # Base Case 1: If all data points belong to the same class\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # Base Case 2: If there are no features left to split on or if maximum depth is reached\n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        # Step 4: Choose the best feature to split the data based on Information Gain\n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude the target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        # Step 5: Create a subtree for each unique value of the best feature\n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "        for value in feature_values:\n",
    "            subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "            tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def predict(self, row, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "\n",
    "        # If it's a leaf node, return the predicted class\n",
    "        if isinstance(tree, (str, int, float)):\n",
    "            return tree\n",
    "        \n",
    "        # Traverse the tree recursively\n",
    "        feature = list(tree.keys())[0]\n",
    "        feature_value = row[feature]\n",
    "        if feature_value in tree[feature]:\n",
    "            return self.predict(row, tree[feature][feature_value])\n",
    "        else:\n",
    "            return None  # If the value is not in the tree, return None (you can modify this behavior)\n",
    "\n",
    "    def predict_all(self, data):\n",
    "        return data.apply(lambda row: self.predict(row), axis=1)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Load your data\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Step 2: Train TDIDT model\n",
    "    model = TDIDT(max_depth=3)  # You can adjust the max_depth parameter as needed\n",
    "    model.fit(data)\n",
    "\n",
    "    # Step 3: Predict on a new row\n",
    "    new_row = data.iloc[0]  # You can replace this with any row from your dataset\n",
    "    prediction = model.predict(new_row)\n",
    "    print(\"Prediction for the first row:\", prediction)\n",
    "\n",
    "    # Step 4: Predict on the entire dataset\n",
    "    predictions = model.predict_all(data)\n",
    "    print(\"Predictions on the dataset:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b38d472-5c0d-4f5e-baac-5c299ad7d1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take First Rule:\n",
      "Buying: med\n",
      "Price: med\n",
      "Maintenance: med\n",
      "Doors: 2\n",
      "Persons: 4\n",
      "Lug_Boot: big\n",
      "\n",
      "Take Last Rule:\n",
      "Buying: low\n",
      "Price: low\n",
      "Maintenance: low\n",
      "Doors: 4\n",
      "Persons: more\n",
      "Lug_Boot: small\n",
      "\n",
      "Prediction for the first row: vgood\n",
      "\n",
      "Predictions on the dataset: 0     vgood\n",
      "1     vgood\n",
      "2     vgood\n",
      "3     vgood\n",
      "4     vgood\n",
      "5      good\n",
      "6      good\n",
      "7      good\n",
      "8      good\n",
      "9      good\n",
      "10     good\n",
      "11     good\n",
      "12     good\n",
      "13     good\n",
      "14     good\n",
      "15     good\n",
      "16     good\n",
      "17     good\n",
      "18     good\n",
      "19     good\n",
      "20     good\n",
      "21     good\n",
      "22     good\n",
      "23     good\n",
      "24     good\n",
      "25     good\n",
      "26     good\n",
      "27     good\n",
      "28     good\n",
      "29     good\n",
      "dtype: object\n",
      "\n",
      "Total rules in the decision tree: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Step 1: Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain\n",
    "def calculate_information_gain(data, feature):\n",
    "    # Calculate the entropy of the entire dataset\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    # Group by the feature and calculate weighted entropy of each group\n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    # Information Gain = Total Entropy - Weighted Entropy of Feature\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: Apply \"Take First\" and \"Take Last\" Rules\n",
    "class TDIDT:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.rule_count = 0\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        # Base Case 1: If all data points belong to the same class\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # Base Case 2: If there are no features left to split on or if maximum depth is reached\n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        # Step 4: Choose the best feature to split the data based on Information Gain\n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude the target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        # Step 5: Create a subtree for each unique value of the best feature\n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "        for value in feature_values:\n",
    "            subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "            tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "            self.rule_count += 1  # Increment rule count\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def predict(self, row, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "\n",
    "        # If it's a leaf node, return the predicted class\n",
    "        if isinstance(tree, (str, int, float)):\n",
    "            return tree\n",
    "        \n",
    "        # Traverse the tree recursively\n",
    "        feature = list(tree.keys())[0]\n",
    "        feature_value = row[feature]\n",
    "        if feature_value in tree[feature]:\n",
    "            return self.predict(row, tree[feature][feature_value])\n",
    "        else:\n",
    "            return None  # If the value is not in the tree, return None (you can modify this behavior)\n",
    "\n",
    "    def predict_all(self, data):\n",
    "        return data.apply(lambda row: self.predict(row), axis=1)\n",
    "\n",
    "    def get_rule_count(self):\n",
    "        return self.rule_count\n",
    "\n",
    "\n",
    "# Apply \"Take First\" and \"Take Last\" rules to the dataset\n",
    "def apply_take_first_last(data):\n",
    "    # Take First: The first row of the dataset\n",
    "    first_row = data.iloc[0]\n",
    "\n",
    "    # Take Last: The last row of the dataset\n",
    "    last_row = data.iloc[-1]\n",
    "\n",
    "    # Print the \"Take First\" and \"Take Last\" rules for each attribute\n",
    "    print(\"Take First Rule:\")\n",
    "    for column in data.columns[:-1]:  # Exclude the target column\n",
    "        print(f\"{column}: {first_row[column]}\")\n",
    "\n",
    "    print(\"\\nTake Last Rule:\")\n",
    "    for column in data.columns[:-1]:  # Exclude the target column\n",
    "        print(f\"{column}: {last_row[column]}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Load your data\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Step 2: Apply Take First and Take Last rules\n",
    "    apply_take_first_last(data)\n",
    "\n",
    "    # Step 3: Train TDIDT model\n",
    "    model = TDIDT(max_depth=3)  # You can adjust the max_depth parameter as needed\n",
    "    model.fit(data)\n",
    "\n",
    "    # Step 4: Predict on a new row\n",
    "    new_row = data.iloc[0]  # You can replace this with any row from your dataset\n",
    "    prediction = model.predict(new_row)\n",
    "    print(\"\\nPrediction for the first row:\", prediction)\n",
    "\n",
    "    # Step 5: Predict on the entire dataset\n",
    "    predictions = model.predict_all(data)\n",
    "    print(\"\\nPredictions on the dataset:\", predictions)\n",
    "\n",
    "    # Step 6: Get the total number of rules in the decision tree\n",
    "    total_rules = model.get_rule_count()\n",
    "    print(\"\\nTotal rules in the decision tree:\", total_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74850228-3316-4e83-8ae8-26b2551ad42d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Save and render the decision tree to a file\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/save/decision_tree\u001b[39m\u001b[38;5;124m'\u001b[39m, view\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Change the path as needed\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Predict on a new row\u001b[39;00m\n\u001b[1;32m    110\u001b[0m new_row \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# You can replace this with any row from your dataset\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/rendering.py:118\u001b[0m, in \u001b[0;36mRender.render\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_filepath(outfile)\n\u001b[0;32m--> 118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[1;32m    122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/saving.py:76\u001b[0m, in \u001b[0;36mSave.save\u001b[0;34m(self, filename, directory, skip_existing)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_existing \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filepath):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filepath\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mkdirs(filepath)\n\u001b[1;32m     78\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrite lines to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m fd:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:49\u001b[0m, in \u001b[0;36mmkdirs\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     48\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.makedirs(\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, dirname)\n\u001b[0;32m---> 49\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(dirname, mode\u001b[38;5;241m=\u001b[39mmode, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/path'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Step 1: Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain using Entropy\n",
    "def calculate_information_gain(data, feature):\n",
    "    # Calculate the entropy of the entire dataset\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    # Group by the feature and calculate weighted entropy of each group\n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    # Information Gain = Total Entropy - Weighted Entropy of Feature\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: Build the Decision Tree and Visualize it using Entropy\n",
    "class TDIDT:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.graph = Digraph(comment='Decision Tree')\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "        self._visualize_tree(self.tree)\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        # Base Case 1: If all data points belong to the same class\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # Base Case 2: If there are no features left to split on or if maximum depth is reached\n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        # Step 4: Choose the best feature to split the data based on Information Gain\n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude the target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        # Step 5: Create a subtree for each unique value of the best feature\n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "        for value in feature_values:\n",
    "            subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "            tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def _visualize_tree(self, tree, parent_name=None):\n",
    "        if isinstance(tree, (str, int, float)):  # It's a leaf node\n",
    "            self.graph.node(name=str(tree), label=str(tree), shape='ellipse', color='lightblue')\n",
    "            if parent_name:\n",
    "                self.graph.edge(parent_name, str(tree))\n",
    "            return str(tree)\n",
    "        \n",
    "        # For non-leaf node, it will be an attribute\n",
    "        feature = list(tree.keys())[0]\n",
    "        for feature_value, subtree in tree[feature].items():\n",
    "            node_name = f\"{feature}_{feature_value}\"\n",
    "            self.graph.node(name=node_name, label=f\"{feature} = {feature_value}\", shape='box', color='lightgreen')\n",
    "            if parent_name:\n",
    "                self.graph.edge(parent_name, node_name)\n",
    "            self._visualize_tree(subtree, node_name)\n",
    "\n",
    "    def predict(self, row, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "\n",
    "        # If it's a leaf node, return the predicted class\n",
    "        if isinstance(tree, (str, int, float)):\n",
    "            return tree\n",
    "        \n",
    "        # Traverse the tree recursively\n",
    "        feature = list(tree.keys())[0]\n",
    "        feature_value = row[feature]\n",
    "        if feature_value in tree[feature]:\n",
    "            return self.predict(row, tree[feature][feature_value])\n",
    "        else:\n",
    "            return None  # If the value is not in the tree, return None (you can modify this behavior)\n",
    "\n",
    "    def predict_all(self, data):\n",
    "        return data.apply(lambda row: self.predict(row), axis=1)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Train TDIDT model\n",
    "    model = TDIDT(max_depth=3)  # You can adjust the max_depth parameter as needed\n",
    "    model.fit(data)\n",
    "\n",
    "    # Save and render the decision tree to a file\n",
    "    model.graph.render('/path/to/save/decision_tree', view=True)  # Change the path as needed\n",
    "\n",
    "    # Predict on a new row\n",
    "    new_row = data.iloc[0]  # You can replace this with any row from your dataset\n",
    "    prediction = model.predict(new_row)\n",
    "    print(\"Prediction for the first row:\", prediction)\n",
    "\n",
    "    # Predict on the entire dataset\n",
    "    predictions = model.predict_all(data)\n",
    "    print(\"Predictions on the dataset:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60c7474b-ddac-4420-99b0-a0cc40d6ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de80609e-4d09-4ad6-81ca-66ec3600c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take First Rule:\n",
      "Buying: med\n",
      "Price: med\n",
      "Maintenance: med\n",
      "Doors: 2\n",
      "Persons: 4\n",
      "Lug_Boot: big\n",
      "\n",
      "Take Last Rule:\n",
      "Buying: low\n",
      "Price: low\n",
      "Maintenance: low\n",
      "Doors: 4\n",
      "Persons: more\n",
      "Lug_Boot: small\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/Users/rahatrihan/Desktop/decision_tree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Save and render the decision tree to a file\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/rahatrihan/Desktop/decision_tree\u001b[39m\u001b[38;5;124m'\u001b[39m, view\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Change the path if needed\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Print the total number of rules generated\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of rules generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mget_rule_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/rendering.py:118\u001b[0m, in \u001b[0;36mRender.render\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_filepath(outfile)\n\u001b[0;32m--> 118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[1;32m    122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/saving.py:79\u001b[0m, in \u001b[0;36mSave.save\u001b[0;34m(self, filename, directory, skip_existing)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mkdirs(filepath)\n\u001b[1;32m     78\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrite lines to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m uline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m     81\u001b[0m         fd\u001b[38;5;241m.\u001b[39mwrite(uline)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/Users/rahatrihan/Desktop/decision_tree'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Step 1: Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain using Entropy\n",
    "def calculate_information_gain(data, feature):\n",
    "    # Calculate the entropy of the entire dataset\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    # Group by the feature and calculate weighted entropy of each group\n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    # Information Gain = Total Entropy - Weighted Entropy of Feature\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: Build the Decision Tree and Visualize it using Entropy\n",
    "class TDIDT:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.rule_count = 0\n",
    "        self.graph = Digraph(comment='Decision Tree')\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "        self._visualize_tree(self.tree)\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        # Base Case 1: If all data points belong to the same class\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # Base Case 2: If there are no features left to split on or if maximum depth is reached\n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        # Step 4: Choose the best feature to split the data based on Information Gain\n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude the target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        # Step 5: Create a subtree for each unique value of the best feature\n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "        for value in feature_values:\n",
    "            subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "            tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "            self.rule_count += 1  # Increment rule count\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def _visualize_tree(self, tree, parent_name=None):\n",
    "        if isinstance(tree, (str, int, float)):  # It's a leaf node\n",
    "            self.graph.node(name=str(tree), label=str(tree), shape='ellipse', color='lightblue')\n",
    "            if parent_name:\n",
    "                self.graph.edge(parent_name, str(tree))\n",
    "            return str(tree)\n",
    "        \n",
    "        # For non-leaf node, it will be an attribute\n",
    "        feature = list(tree.keys())[0]\n",
    "        for feature_value, subtree in tree[feature].items():\n",
    "            node_name = f\"{feature}_{feature_value}\"\n",
    "            self.graph.node(name=node_name, label=f\"{feature} = {feature_value}\", shape='box', color='lightgreen')\n",
    "            if parent_name:\n",
    "                self.graph.edge(parent_name, node_name)\n",
    "            self._visualize_tree(subtree, node_name)\n",
    "\n",
    "    def predict(self, row, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "\n",
    "        # If it's a leaf node, return the predicted class\n",
    "        if isinstance(tree, (str, int, float)):\n",
    "            return tree\n",
    "        \n",
    "        # Traverse the tree recursively\n",
    "        feature = list(tree.keys())[0]\n",
    "        feature_value = row[feature]\n",
    "        if feature_value in tree[feature]:\n",
    "            return self.predict(row, tree[feature][feature_value])\n",
    "        else:\n",
    "            return None  # If the value is not in the tree, return None (you can modify this behavior)\n",
    "\n",
    "    def predict_all(self, data):\n",
    "        return data.apply(lambda row: self.predict(row), axis=1)\n",
    "\n",
    "    def get_rule_count(self):\n",
    "        return self.rule_count\n",
    "\n",
    "\n",
    "# Step 4: Apply \"Take First\" and \"Take Last\" rules to the dataset\n",
    "def apply_take_first_last(data):\n",
    "    # Take First: The first row of the dataset\n",
    "    first_row = data.iloc[0]\n",
    "\n",
    "    # Take Last: The last row of the dataset\n",
    "    last_row = data.iloc[-1]\n",
    "\n",
    "    # Print the \"Take First\" and \"Take Last\" rules for each attribute\n",
    "    print(\"Take First Rule:\")\n",
    "    for column in data.columns[:-1]:  # Exclude the target column\n",
    "        print(f\"{column}: {first_row[column]}\")\n",
    "\n",
    "    print(\"\\nTake Last Rule:\")\n",
    "    for column in data.columns[:-1]:  # Exclude the target column\n",
    "        print(f\"{column}: {last_row[column]}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'  # Change path if needed\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Apply \"Take First\" and \"Take Last\" rules\n",
    "    apply_take_first_last(data)\n",
    "\n",
    "    # Train TDIDT model\n",
    "    model = TDIDT(max_depth=None)  # Set max_depth as needed\n",
    "    model.fit(data)\n",
    "\n",
    "    # Save and render the decision tree to a file\n",
    "    model.graph.render('/Users/rahatrihan/Desktop/decision_tree', view=True)  # Change the path if needed\n",
    "\n",
    "    # Print the total number of rules generated\n",
    "    print(f\"Total number of rules generated: {model.get_rule_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26cf9475-828b-4952-8733-9f3efb335e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Approach  Number of Rules  \\\n",
      "0  Take First               20   \n",
      "1   Take Last               20   \n",
      "\n",
      "                     Generated Decision Tree (Rules)  \n",
      "0  {'Buying': {'med': 'vgood', 'low': {'Price': {...  \n",
      "1  {'Buying': {'med': 'vgood', 'low': {'Price': {...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Step 1: Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain using Entropy\n",
    "def calculate_information_gain(data, feature):\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: TDIDT Algorithm with 'Take First' and 'Take Last'\n",
    "class TDIDT:\n",
    "    def __init__(self, approach='take_first', max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.rule_count = 0\n",
    "        self.approach = approach\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "        return self.tree\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "\n",
    "        # Using the 'Take First' or 'Take Last' approach to build the tree\n",
    "        if self.approach == 'take_first':\n",
    "            # Select the first row for splitting decision\n",
    "            first_row = data.iloc[0]\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "\n",
    "        elif self.approach == 'take_last':\n",
    "            # Select the last row for splitting decision\n",
    "            last_row = data.iloc[-1]\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "        \n",
    "        return tree\n",
    "\n",
    "# Step 4: Calculate and Display the Number of Rules in a Table\n",
    "def display_rules_table(take_first_tree, take_last_tree):\n",
    "    # Create a dictionary to store the results\n",
    "    rules = {\n",
    "        \"Approach\": [\"Take First\", \"Take Last\"],\n",
    "        \"Number of Rules\": [take_first_tree.rule_count, take_last_tree.rule_count],\n",
    "        \"Generated Decision Tree (Rules)\": [str(take_first_tree.tree), str(take_last_tree.tree)]\n",
    "    }\n",
    "    \n",
    "    # Convert dictionary to a DataFrame for better presentation\n",
    "    df = pd.DataFrame(rules)\n",
    "    return df\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset (replace with the correct path)\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Train TDIDT model using 'Take First' and 'Take Last' approaches\n",
    "    model_take_first = TDIDT(approach='take_first')\n",
    "    model_take_first.fit(data)\n",
    "    \n",
    "    model_take_last = TDIDT(approach='take_last')\n",
    "    model_take_last.fit(data)\n",
    "\n",
    "    # Display the number of rules in a table\n",
    "    result_table = display_rules_table(model_take_first, model_take_last)\n",
    "    print(result_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0577ac2-34a8-4ea0-87f7-d47ca95897f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree using 'Take First' approach:\n",
      "Buying?\n",
      "  med -> vgood\n",
      "  low ->     Price?\n",
      "      low ->         Maintenance?\n",
      "          low ->             Doors?\n",
      "              5more ->                 Persons?\n",
      "                  2 -> unacc\n",
      "                  4 ->                     Lug_Boot?\n",
      "                      small -> good\n",
      "                      big -> unacc\n",
      "                  more -> unacc\n",
      "              4 ->                 Persons?\n",
      "                  4 -> good\n",
      "                  more ->                     Lug_Boot?\n",
      "                      small -> acc\n",
      "                      med -> good\n",
      "              2 ->                 Persons?\n",
      "                  4 ->                     Lug_Boot?\n",
      "                      med -> good\n",
      "                      small -> acc\n",
      "              3 -> acc\n",
      "\n",
      "Decision Tree using 'Take Last' approach:\n",
      "Buying?\n",
      "  med -> vgood\n",
      "  low ->     Price?\n",
      "      low ->         Maintenance?\n",
      "          low ->             Doors?\n",
      "              5more ->                 Persons?\n",
      "                  2 -> unacc\n",
      "                  4 ->                     Lug_Boot?\n",
      "                      small -> good\n",
      "                      big -> unacc\n",
      "                  more -> unacc\n",
      "              4 ->                 Persons?\n",
      "                  4 -> good\n",
      "                  more ->                     Lug_Boot?\n",
      "                      small -> acc\n",
      "                      med -> good\n",
      "              2 ->                 Persons?\n",
      "                  4 ->                     Lug_Boot?\n",
      "                      med -> good\n",
      "                      small -> acc\n",
      "              3 -> acc\n",
      "\n",
      "Number of Rules Generated:\n",
      "     Approach  Number of Rules  \\\n",
      "0  Take First               20   \n",
      "1   Take Last               20   \n",
      "\n",
      "                     Generated Decision Tree (Rules)  \n",
      "0  {'Buying': {'med': 'vgood', 'low': {'Price': {...  \n",
      "1  {'Buying': {'med': 'vgood', 'low': {'Price': {...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Step 1: Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain using Entropy\n",
    "def calculate_information_gain(data, feature):\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: TDIDT Algorithm with 'Take First' and 'Take Last'\n",
    "class TDIDT:\n",
    "    def __init__(self, approach='take_first', max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.rule_count = 0\n",
    "        self.approach = approach\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "        return self.tree\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        # If all instances have the same label, return the label\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # If there are no more features to split or max depth is reached, return the majority label\n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "\n",
    "        # Using the 'Take First' or 'Take Last' approach to build the tree\n",
    "        if self.approach == 'take_first':\n",
    "            first_row = data.iloc[0]\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "\n",
    "        elif self.approach == 'take_last':\n",
    "            last_row = data.iloc[-1]\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def display_tree(self, tree=None, indent=\"\"):\n",
    "        \"\"\"Recursively prints the decision tree.\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "        \n",
    "        for feature, branches in tree.items():\n",
    "            print(f\"{indent}{feature}?\")\n",
    "            for value, subtree in branches.items():\n",
    "                if isinstance(subtree, dict):  # if it's a branch (not a leaf)\n",
    "                    print(f\"{indent}  {value} -> \", end=\"\")\n",
    "                    self.display_tree(subtree, indent + \"    \")\n",
    "                else:  # if it's a leaf (final classification)\n",
    "                    print(f\"{indent}  {value} -> {subtree}\")\n",
    "\n",
    "# Step 4: Calculate and Display the Number of Rules in a Table\n",
    "def display_rules_table(take_first_tree, take_last_tree):\n",
    "    rules = {\n",
    "        \"Approach\": [\"Take First\", \"Take Last\"],\n",
    "        \"Number of Rules\": [take_first_tree.rule_count, take_last_tree.rule_count],\n",
    "        \"Generated Decision Tree (Rules)\": [str(take_first_tree.tree), str(take_last_tree.tree)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(rules)\n",
    "    return df\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset (replace with the correct path)\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Train TDIDT model using 'Take First' and 'Take Last' approaches\n",
    "    model_take_first = TDIDT(approach='take_first')\n",
    "    model_take_first.fit(data)\n",
    "    \n",
    "    model_take_last = TDIDT(approach='take_last')\n",
    "    model_take_last.fit(data)\n",
    "\n",
    "    # Display the generated trees\n",
    "    print(\"Decision Tree using 'Take First' approach:\")\n",
    "    model_take_first.display_tree()\n",
    "\n",
    "    print(\"\\nDecision Tree using 'Take Last' approach:\")\n",
    "    model_take_last.display_tree()\n",
    "\n",
    "    # Display the number of rules in a table\n",
    "    result_table = display_rules_table(model_take_first, model_take_last)\n",
    "    print(\"\\nNumber of Rules Generated:\")\n",
    "    print(result_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95a2b3ac-051f-4b82-a4f3-46579daf6469",
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/backend/execute.py:78\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1955\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m model_take_first\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Render and save the decision tree as a PNG file\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m model_take_first\u001b[38;5;241m.\u001b[39mrender_tree(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree_take_first\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Save as PNG\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Train TDIDT model using 'Take Last' approach\u001b[39;00m\n\u001b[1;32m    116\u001b[0m model_take_last \u001b[38;5;241m=\u001b[39m TDIDT(approach\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtake_last\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 100\u001b[0m, in \u001b[0;36mTDIDT.render_tree\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Render the decision tree to a file.\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_tree(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree, graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot\u001b[38;5;241m.\u001b[39mrender(filename, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, cleanup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[0;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/backend/rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 326\u001b[0m execute\u001b[38;5;241m.\u001b[39mrun_check(cmd,\n\u001b[1;32m    327\u001b[0m                   cwd\u001b[38;5;241m=\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparts \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    328\u001b[0m                   quiet\u001b[38;5;241m=\u001b[39mquiet,\n\u001b[1;32m    329\u001b[0m                   capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/backend/execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Step 1: Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain using Entropy\n",
    "def calculate_information_gain(data, feature):\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: TDIDT Algorithm with 'Take First' and 'Take Last'\n",
    "class TDIDT:\n",
    "    def __init__(self, approach='take_first', max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.rule_count = 0\n",
    "        self.approach = approach\n",
    "        self.dot = Digraph()\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "        return self.tree\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        # If all instances have the same label, return the label\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # If there are no more features to split or max depth is reached, return the majority label\n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "\n",
    "        # Using the 'Take First' or 'Take Last' approach to build the tree\n",
    "        if self.approach == 'take_first':\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "\n",
    "        elif self.approach == 'take_last':\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def display_tree(self, tree=None, parent=None, graph=None):\n",
    "        \"\"\"Recursively adds nodes and edges to the graphviz Digraph.\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "\n",
    "        if graph is None:\n",
    "            graph = self.dot\n",
    "\n",
    "        # If it's a dictionary, it means it's a non-leaf node\n",
    "        if isinstance(tree, dict):\n",
    "            for feature, branches in tree.items():\n",
    "                # For each feature, create a node\n",
    "                node_name = feature\n",
    "                graph.node(node_name, label=feature)\n",
    "                if parent:\n",
    "                    graph.edge(parent, node_name)\n",
    "\n",
    "                for value, subtree in branches.items():\n",
    "                    child_node_name = f\"{feature}_{value}\"\n",
    "                    graph.node(child_node_name, label=f\"{value}\")\n",
    "                    graph.edge(node_name, child_node_name)\n",
    "                    self.display_tree(subtree, child_node_name, graph)\n",
    "        else:\n",
    "            # If it's a leaf node (final class label)\n",
    "            graph.node(f\"{parent}_leaf\", label=f\"{tree}\")\n",
    "            graph.edge(parent, f\"{parent}_leaf\")\n",
    "        \n",
    "        return graph\n",
    "\n",
    "    def render_tree(self, filename='decision_tree'):\n",
    "        \"\"\"Render the decision tree to a file.\"\"\"\n",
    "        self.dot = self.display_tree(self.tree, graph=self.dot)\n",
    "        self.dot.render(filename, format='png', cleanup=True)\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset (replace with the correct path)\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Train TDIDT model using 'Take First' approach\n",
    "    model_take_first = TDIDT(approach='take_first')\n",
    "    model_take_first.fit(data)\n",
    "    \n",
    "    # Render and save the decision tree as a PNG file\n",
    "    model_take_first.render_tree('decision_tree_take_first')  # Save as PNG\n",
    "\n",
    "    # Train TDIDT model using 'Take Last' approach\n",
    "    model_take_last = TDIDT(approach='take_last')\n",
    "    model_take_last.fit(data)\n",
    "    \n",
    "    # Render and save the decision tree as a PNG file\n",
    "    model_take_last.render_tree('decision_tree_take_last')  # Save as PNG\n",
    "\n",
    "    print(\"Decision trees have been generated and saved as PNG files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08918fb7-73ba-460f-a0da-0f81c6ca00fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bash' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m/\u001b[39mbash(\u001b[38;5;241m-\u001b[39mc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$(curl, -fsSL, https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bash' is not defined"
     ]
    }
   ],
   "source": [
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b912f2eb-b305-4478-a6ef-2426878bace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules generated (Take First): 21\n"
     ]
    },
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/backend/execute.py:78\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1955\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m model_take_first\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rules generated (Take First): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_take_first\u001b[38;5;241m.\u001b[39mrule_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m model_take_first\u001b[38;5;241m.\u001b[39mrender_tree(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree_take_first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Train TDIDT model using 'Take Last' approach\u001b[39;00m\n\u001b[1;32m    115\u001b[0m model_take_last \u001b[38;5;241m=\u001b[39m TDIDT(approach\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtake_last\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 100\u001b[0m, in \u001b[0;36mTDIDT.render_tree\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Render the decision tree to a file.\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_tree(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree, graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot\u001b[38;5;241m.\u001b[39mrender(filename, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, cleanup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[0;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/backend/rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 326\u001b[0m execute\u001b[38;5;241m.\u001b[39mrun_check(cmd,\n\u001b[1;32m    327\u001b[0m                   cwd\u001b[38;5;241m=\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparts \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    328\u001b[0m                   quiet\u001b[38;5;241m=\u001b[39mquiet,\n\u001b[1;32m    329\u001b[0m                   capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphviz/backend/execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Calculate Information Gain using Entropy\n",
    "def calculate_information_gain(data, feature):\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# TDIDT Algorithm with 'Take First' and 'Take Last'\n",
    "class TDIDT:\n",
    "    def __init__(self, approach='take_first', max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.rule_count = 0\n",
    "        self.approach = approach\n",
    "        self.dot = Digraph()\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "        return self.tree\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        # If all instances have the same label, return the label\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # If there are no more features to split or max depth is reached, return the majority label\n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "\n",
    "        # Using the 'Take First' or 'Take Last' approach to build the tree\n",
    "        if self.approach == 'take_first':\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "\n",
    "        elif self.approach == 'take_last':\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def display_tree(self, tree=None, parent=None, graph=None):\n",
    "        \"\"\"Recursively adds nodes and edges to the graphviz Digraph.\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "\n",
    "        if graph is None:\n",
    "            graph = self.dot\n",
    "\n",
    "        # If it's a dictionary, it means it's a non-leaf node\n",
    "        if isinstance(tree, dict):\n",
    "            for feature, branches in tree.items():\n",
    "                # For each feature, create a node\n",
    "                node_name = feature\n",
    "                graph.node(node_name, label=feature)\n",
    "                if parent:\n",
    "                    graph.edge(parent, node_name)\n",
    "\n",
    "                for value, subtree in branches.items():\n",
    "                    child_node_name = f\"{feature}_{value}\"\n",
    "                    graph.node(child_node_name, label=f\"{value}\")\n",
    "                    graph.edge(node_name, child_node_name)\n",
    "                    self.display_tree(subtree, child_node_name, graph)\n",
    "        else:\n",
    "            # If it's a leaf node (final class label)\n",
    "            graph.node(f\"{parent}_leaf\", label=f\"{tree}\")\n",
    "            graph.edge(parent, f\"{parent}_leaf\")\n",
    "        \n",
    "        return graph\n",
    "\n",
    "    def render_tree(self, filename='decision_tree'):\n",
    "        \"\"\"Render the decision tree to a file.\"\"\"\n",
    "        self.dot = self.display_tree(self.tree, graph=self.dot)\n",
    "        self.dot.render(filename, format='png', cleanup=True)\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset (replace with the correct path)\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Train TDIDT model using 'Take First' approach\n",
    "    model_take_first = TDIDT(approach='take_first')\n",
    "    model_take_first.fit(data)\n",
    "    print(f\"Number of rules generated (Take First): {model_take_first.rule_count}\")\n",
    "    model_take_first.render_tree('decision_tree_take_first')\n",
    "\n",
    "    # Train TDIDT model using 'Take Last' approach\n",
    "    model_take_last = TDIDT(approach='take_last')\n",
    "    model_take_last.fit(data)\n",
    "    print(f\"Number of rules generated (Take Last): {model_take_last.rule_count}\")\n",
    "    model_take_last.render_tree('decision_tree_take_last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ccd4ac89-8432-4f83-9bbb-f17773deca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /opt/anaconda3/lib/python3.12/site-packages (0.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a04fe3cf-8e83-4935-812c-af1229fdd853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules generated (Take First approach): 12\n",
      "Number of rules generated (Take Last approach): 12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Step 1: Calculate Entropy\n",
    "def calculate_entropy(data):\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain using Entropy\n",
    "def calculate_information_gain(data, feature):\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    \n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: TDIDT Algorithm with 'Take First' and 'Take Last'\n",
    "class TDIDT:\n",
    "    def __init__(self, approach='take_first', max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.rule_count = 0\n",
    "        self.approach = approach\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "        return self.tree\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        # If all instances have the same label, return the label\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # If there are no more features to split or max depth is reached, return the majority label\n",
    "        if len(data.columns) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        for feature in data.columns[:-1]:  # Exclude target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "\n",
    "        # Using the 'Take First' or 'Take Last' approach to build the tree\n",
    "        if self.approach == 'take_first':\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "\n",
    "        elif self.approach == 'take_last':\n",
    "            for value in feature_values:\n",
    "                subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "                tree[best_feature][value] = self._build_tree(subset, depth + 1)\n",
    "                self.rule_count += 1\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def count_rules(self, tree=None):\n",
    "        \"\"\"Recursively count the number of leaf nodes (rules) in the tree.\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "        \n",
    "        rule_count = 0\n",
    "        \n",
    "        if isinstance(tree, dict):  # Non-leaf node\n",
    "            for feature, branches in tree.items():\n",
    "                for value, subtree in branches.items():\n",
    "                    rule_count += self.count_rules(subtree)  # Recursive call\n",
    "        else:\n",
    "            rule_count += 1  # Leaf node, one rule\n",
    "        \n",
    "        return rule_count\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset (replace with the correct path)\n",
    "    file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Train TDIDT model using 'Take First' approach\n",
    "    model_take_first = TDIDT(approach='take_first')\n",
    "    model_take_first.fit(data)\n",
    "    rules_take_first = model_take_first.count_rules()\n",
    "\n",
    "    # Train TDIDT model using 'Take Last' approach\n",
    "    model_take_last = TDIDT(approach='take_last')\n",
    "    model_take_last.fit(data)\n",
    "    rules_take_last = model_take_last.count_rules()\n",
    "\n",
    "    print(f\"Number of rules generated (Take First approach): {rules_take_first}\")\n",
    "    print(f\"Number of rules generated (Take Last approach): {rules_take_last}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53189dae-826a-4bb6-9219-6e1c290bbb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buying = med?\n",
      "  Predict: vgood\n",
      "Buying = low?\n",
      "  Price = low?\n",
      "    Maintenance = low?\n",
      "      Doors = 5more?\n",
      "        Persons = 2?\n",
      "          Predict: unacc\n",
      "        Persons = 4?\n",
      "          Lug_Boot = small?\n",
      "            Predict: good\n",
      "          Lug_Boot = big?\n",
      "            Predict: unacc\n",
      "        Persons = more?\n",
      "          Predict: unacc\n",
      "      Doors = 4?\n",
      "        Persons = 4?\n",
      "          Predict: good\n",
      "        Persons = more?\n",
      "          Lug_Boot = small?\n",
      "            Predict: acc\n",
      "          Lug_Boot = med?\n",
      "            Predict: good\n",
      "      Doors = 2?\n",
      "        Persons = 4?\n",
      "          Lug_Boot = med?\n",
      "            Predict: good\n",
      "          Lug_Boot = small?\n",
      "            Predict: acc\n",
      "      Doors = 3?\n",
      "        Predict: acc\n",
      "  Price = med?\n",
      "    Predict: good\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Step 1: Calculate Entropy for a dataset\n",
    "def calculate_entropy(data):\n",
    "    # Calculate the proportion of each class\n",
    "    labels = data.iloc[:, -1].value_counts(normalize=True)\n",
    "    # Calculate entropy: H(D) = - sum(p(x) * log2(p(x))) for all x in labels\n",
    "    entropy = -sum(labels * labels.apply(lambda x: math.log2(x) if x > 0 else 0))\n",
    "    return entropy\n",
    "\n",
    "# Step 2: Calculate Information Gain for a feature\n",
    "def calculate_information_gain(data, feature):\n",
    "    total_entropy = calculate_entropy(data)\n",
    "    feature_values = data[feature].value_counts(normalize=True)\n",
    "    weighted_entropy = sum(feature_values.apply(lambda val: calculate_entropy(data[data[feature] == val])) * feature_values)\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Step 3: TDIDT Algorithm to build a Decision Tree\n",
    "class TDIDT:\n",
    "    def __init__(self, approach='take_first'):\n",
    "        self.tree = None\n",
    "        self.approach = approach\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.tree = self._build_tree(data)\n",
    "        return self.tree\n",
    "\n",
    "    def _build_tree(self, data):\n",
    "        # If all instances have the same label, return the label\n",
    "        if len(data.iloc[:, -1].unique()) == 1:\n",
    "            return data.iloc[0, -1]\n",
    "        \n",
    "        # If there are no more features to split on, return the majority label\n",
    "        if len(data.columns) == 1:\n",
    "            return data.iloc[:, -1].mode()[0]\n",
    "        \n",
    "        best_feature = None\n",
    "        max_info_gain = -1\n",
    "        \n",
    "        # Find the best feature to split on\n",
    "        for feature in data.columns[:-1]:  # Exclude target column\n",
    "            info_gain = calculate_information_gain(data, feature)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        tree = {best_feature: {}}\n",
    "        feature_values = data[best_feature].unique()\n",
    "\n",
    "        # Recursively build the tree based on the selected feature\n",
    "        for value in feature_values:\n",
    "            subset = data[data[best_feature] == value].drop(columns=[best_feature])\n",
    "            tree[best_feature][value] = self._build_tree(subset)\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\"\"):\n",
    "        \"\"\"Print the decision tree in a human-readable format.\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "        \n",
    "        if isinstance(tree, dict):\n",
    "            for feature, branches in tree.items():\n",
    "                for value, subtree in branches.items():\n",
    "                    print(f\"{indent}{feature} = {value}?\")\n",
    "                    self.print_tree(subtree, indent + \"  \")\n",
    "        else:\n",
    "            print(f\"{indent}Predict: {tree}\")\n",
    "\n",
    "# Load the dataset (make sure to replace with your dataset path)\n",
    "file_path = '/Users/rahatrihan/Desktop/AIUB/data mining/Assignment/car_evaluation.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Train a TDIDT model using 'Take First' approach\n",
    "model = TDIDT(approach='take_first')\n",
    "model.fit(data)\n",
    "\n",
    "# Print the generated decision tree\n",
    "model.print_tree()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6b2a3-e66e-464a-b818-c521d8d1d902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
